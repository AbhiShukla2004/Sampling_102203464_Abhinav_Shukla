# -*- coding: utf-8 -*-
"""Assignment_Sampling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvQXVDvG_9YLt98i9IP7S8KsXDDFyUxk
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN

# Load the dataset into a DataFrame
data_frame = pd.read_csv("Creditcard_data.csv")

# Function to balance the dataset using oversampling
def make_dataset_balanced(data_frame):
    features = data_frame.iloc[:, :-1]
    labels = data_frame.iloc[:, -1]
    oversampler = RandomOverSampler()
    features_balanced, labels_balanced = oversampler.fit_resample(features, labels)
    return pd.concat([features_balanced, labels_balanced], axis=1)

balanced_data_frame = make_dataset_balanced(data_frame)

# Function to determine sample size (e.g., 10% of the total data)
def compute_sample_size(data_frame, fraction=0.1):
    return int(len(data_frame) * fraction)

sample_count = compute_sample_size(balanced_data_frame)

# Create multiple samples from the balanced dataset
sample_list = [balanced_data_frame.sample(sample_count) for _ in range(5)]

# Function to apply different sampling methods
def apply_sampling_method(data_frame, method):
    features = data_frame.iloc[:, :-1]
    labels = data_frame.iloc[:, -1]
    if method == "oversample":
        sampler = RandomOverSampler()
    elif method == "undersample":
        sampler = RandomUnderSampler()
    elif method == "smoteenn":
        sampler = SMOTEENN()
    else:
        return data_frame
    features_sampled, labels_sampled = sampler.fit_resample(features, labels)
    return pd.concat([features_sampled, labels_sampled], axis=1)

methods = ["oversample", "undersample", "smoteenn"]
sampled_datasets = [apply_sampling_method(sample, methods[i % len(methods)]) for i, sample in enumerate(sample_list)]

# Define machine learning models
ml_models = {
    "RandomForest": RandomForestClassifier(),
    "GradientBoosting": GradientBoostingClassifier(),
    "SVM": SVC(),
    "LogisticRegression": LogisticRegression(),
    "KNN": KNeighborsClassifier()
}

# Train and test models on the sampled datasets
performance_results = []
for idx, dataset in enumerate(sampled_datasets):
    features = dataset.iloc[:, :-1]
    labels = dataset.iloc[:, -1]
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)
    for model_name, model_instance in ml_models.items():
        model_instance.fit(X_train, y_train)
        predictions = model_instance.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        performance_results.append({"Sample": f"Sample{idx+1}", "Sampling Method": methods[idx % len(methods)], "Model": model_name, "Accuracy": accuracy})

# Save results to a CSV file
results_df = pd.DataFrame(performance_results)
results_df.to_csv("sampling_model_results.csv", index=False)

# Display performance for all combinations
print("Performance results saved to 'sampling_model_results.csv'")